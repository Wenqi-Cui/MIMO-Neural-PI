{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD52deps8Lra"
      },
      "source": [
        "# Install package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqD27N96mzOA"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPpSGDcHm5T3"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPdvp47f8Lrb",
        "scrolled": true
      },
      "source": [
        "# A RNN-based Reinforcement Learning Framework for Frequency Control Problem with Stability Guarantee\n",
        "import collections\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import numpy as np\n",
        "import gym\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import random\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.keras.layers import RNN\n",
        "import tensorflow.keras.backend as K\n",
        "import sys\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import copy\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import networkx as nx\n",
        "import scipy\n",
        "import pickle\n",
        "import time\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from datetime import datetime\n",
        "random.seed(datetime.now())"
      ],
      "metadata": {
        "id": "Ja-GgbMSxr4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGE94Q4GABya"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEdncMMcmduT",
        "scrolled": true
      },
      "source": [
        "class Traffic():\n",
        "    def  __init__(self, v0, v1, v_ref, k, delta_t, dim_action, max_action_edge, \n",
        "                  dim_edge, Penalty_action, coef_cost, incidence,  incidence_communication, feedbackfun):\n",
        "        self.param_gamma=1\n",
        "        self.v0 = v0.reshape((1, -1)).astype(np.float32)\n",
        "        self.v1 = v1.astype(np.float32)\n",
        "        self.state_v_ref = v_ref.reshape((1, -1)).astype(np.float32)\n",
        "        self.diag_v1 = np.diag(v1).astype(np.float32)\n",
        "        self.diag_K = np.diag(k).astype(np.float32)\n",
        "        self.delta_t = delta_t\n",
        "        self.dim_action = dim_action\n",
        "        self.max_action_edge = max_action_edge.reshape((1, -1)).astype(np.float32)\n",
        "        self.dim_edge = dim_edge\n",
        "        self.Penalty_action = Penalty_action\n",
        "        self.diag_c = np.diag(coef_cost).astype(np.float32)\n",
        "        self.incidence = incidence.astype(np.float32)\n",
        "        self.incidence_communication = incidence_communication.astype(np.float32)\n",
        "        self.eta0 = 2\n",
        "        self.feedbackfun = feedbackfun\n",
        "        self.matrix_grad_action = self.incidence_communication@self.incidence_communication.T@self.diag_c*2\n",
        "       \n",
        "\n",
        "    def step(self, action_integral):\n",
        "        action_network = -self.edgefeedback_function(self.state_eta, func = self.feedbackfun)@(self.incidence.T)\n",
        "        dot_v = ((-self.state_v+self.v0) + (action_network+action_integral)@self.diag_v1)@self.diag_K\n",
        "        dot_eta = self.state_v@self.incidence\n",
        "        cost_action, grad_action = self.calc_grad_action(action_integral)\n",
        "        dot_s = -self.state_v + self.state_v_ref - grad_action@self.incidence@self.incidence.T@self.diag_c*0.1\n",
        "        \n",
        "\n",
        "        self.state_v =  self.state_v + self.delta_t*dot_v        \n",
        "        self.state_s = self.state_s + self.delta_t*dot_s/1\n",
        "        self.state_eta = self.state_eta + self.delta_t*dot_eta\n",
        "        loss = self.param_gamma*pow(-self.state_v+self.state_v_ref,2)\n",
        "        return self.state_s, self.state_v, self.state_eta, loss, action_network\n",
        "    def step_PI(self,  actionP, actionI):\n",
        "        action_network = -self.edgefeedback_function(self.state_eta, func = self.feedbackfun)@(self.incidence.T)\n",
        "        action = actionP + actionI\n",
        "        dot_v = ((-self.state_v+self.v0) + (action_network+action)@self.diag_v1)@self.diag_K\n",
        "        dot_eta = self.state_v@self.incidence\n",
        "        cost_action, grad_action = self.calc_grad_action(actionI)\n",
        "        dot_s = -self.state_v + self.state_v_ref - grad_action@self.matrix_grad_action\n",
        "\n",
        "        self.state_v =  self.state_v + self.delta_t*dot_v        \n",
        "        self.state_s = self.state_s + self.delta_t*dot_s/1\n",
        "        self.state_eta = self.state_eta + self.delta_t*dot_eta\n",
        "        loss = self.param_gamma*pow(-self.state_v+self.state_v_ref,2)\n",
        "        return self.state_s, self.state_v, self.state_eta, loss, action_network   \n",
        "\n",
        "    def step_edge(self, action_edgefeedback, actionP, actionI, print_grad = False):\n",
        "        action_network = -action_edgefeedback@(self.incidence.T)\n",
        "        action = actionP + actionI\n",
        "        dot_v = ((-self.state_v+self.v0) + (action_network+action)@self.diag_v1)@self.diag_K\n",
        "\n",
        "        dot_eta = self.state_v@self.incidence\n",
        "        cost_action, grad_action = self.calc_grad_action(actionI)\n",
        "        if print_grad:\n",
        "            print(grad_action@self.matrix_grad_action)        \n",
        "        dot_s = -self.state_v + self.state_v_ref - grad_action@self.matrix_grad_action\n",
        "        self.state_v =  self.state_v + self.delta_t*dot_v        \n",
        "        self.state_s = self.state_s + self.delta_t*dot_s/1\n",
        "        self.state_eta = self.state_eta + self.delta_t*dot_eta\n",
        "        loss = self.param_gamma*pow(-self.state_v+self.state_v_ref,2)\n",
        "        return self.state_s, self.state_v, self.state_eta, loss, action_network   \n",
        "\n",
        "  \n",
        "\n",
        "    def calc_grad_action(self, action):\n",
        "        return 0.5*(action**2)@self.diag_c, action@self.diag_c\n",
        "\n",
        "    def set_state(self, state_input):\n",
        "        self.state_v = state_input\n",
        "        self.state_eta = self.eta0*np.ones((1,self.dim_edge),dtype=np.float32)\n",
        "        self.state_s = np.zeros((1,self.dim_action),dtype=np.float32)\n",
        "\n",
        "    def edgefeedback_function(self, eta, func = 'poly1/3'):\n",
        "        if  func == 'poly1/3':\n",
        "            y = np.sign(eta-self.eta0)*(abs(eta-self.eta0)**(1/3))\n",
        "        if  func == 'tanh':\n",
        "            y = np.tanh(eta-self.eta0)\n",
        "        if  func == 'poly3':\n",
        "            y = np.sign(eta-self.eta0)*(abs(eta-self.eta0)**(3))\n",
        "\n",
        "\n",
        "        action=self.max_action_edge-np_relu(self.max_action_edge-y)+np_relu(-self.max_action_edge-y)\n",
        "\n",
        "        return action\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.state_v = self.v0\n",
        "        self.state_eta = self.eta0*np.ones((1,self.dim_edge),dtype=np.float32)\n",
        "        self.state_s = np.zeros((1,dim_action),dtype=np.float32)\n",
        "\n",
        "        return self.state_v\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dim_action = 20 #dimension of action space\n",
        "dim_state = dim_action #dimension of state space\n",
        "f = open('env_n20v2.pckl', 'rb')\n",
        "[v0, v1, v_ref, k, delta_t, dim_action, max_action_edge, dim_edge, \n",
        "                Penalty_action, coef_cost, incidence,  incidence_comm]= pickle.load(f)\n",
        "f.close()\n",
        "v_ref_nom = 5.2\n",
        "v_ref = v_ref_nom*np.ones(dim_action,dtype=np.float32)\n",
        "v0_nom =5\n",
        "v1_nom = 1\n",
        "v0_random =1.\n",
        "v1_random = 0.5\n",
        "\n",
        "env = Traffic(v0, v1, v_ref, k, delta_t, dim_action, max_action_edge, dim_edge, \n",
        "                Penalty_action, coef_cost, incidence,  incidence_comm, feedbackfun ='tanh')"
      ],
      "metadata": {
        "id": "WM-_-95KQDb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJNB8I_o0AN0"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### edge + PI ICNN beta"
      ],
      "metadata": {
        "id": "Y3tffH3F0AN3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### edge and P control\n",
        "# RNN Cell to integrate state transition dynamics\n",
        "class MinimalRNNCell_SCNN(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units, action_units_edge, action_units_node_p, action_units_node_i, internal_units,internal_units_ICNN,env,batchsize,**kwargs):\n",
        "        self.units = units\n",
        "        self.action_units_edge = action_units_edge\n",
        "        self.action_units_node_p = action_units_node_p\n",
        "        self.action_units_node_i = action_units_node_i\n",
        "        self.state_size = units\n",
        "        self.internal_units = internal_units\n",
        "        self.internal_units_ICNN = internal_units_ICNN \n",
        "\n",
        "        self.batchsize=batchsize\n",
        "        self.delta_t = tf.constant(env.delta_t,dtype=tf.float32)\n",
        "\n",
        "        self.v0 = tf.constant(env.v0,dtype=tf.float32)\n",
        "        self.v1 = tf.constant(env.v1,dtype=tf.float32)\n",
        "        self.state_v_ref = tf.constant(env.state_v_ref*np.ones((batchsize, action_units_node_p)),dtype=tf.float32)\n",
        "        self.diag_v1 = tf.constant(env.diag_v1,dtype=tf.float32)\n",
        "        self.diag_K = tf.constant(env.diag_K ,dtype=tf.float32)\n",
        "        self.delta_t = tf.constant(env.delta_t ,dtype=tf.float32)\n",
        "        self.dim_action = env.dim_action\n",
        "        self.dim_edge = env.dim_edge\n",
        "        self.eta0 = tf.constant(env.eta0 ,dtype=tf.float32)\n",
        "        self. matrix_grad_action =  tf.constant(env.matrix_grad_action,dtype=tf.float32)\n",
        "\n",
        "\n",
        "        self.Penalty_action = tf.constant(env.Penalty_action ,dtype=tf.float32)\n",
        "        self.diag_c = tf.constant(env.diag_c ,dtype=tf.float32)\n",
        "        self.incidence = tf.constant(env.incidence ,dtype=tf.float32)\n",
        "        self.w_recover =tf.constant(tf.linalg.band_part(-tf.ones((internal_units,internal_units)),0,1)\\\n",
        "                                        +2*tf.eye(internal_units),dtype=tf.float32)\n",
        "        self.b_recover = tf.constant(tf.linalg.band_part(tf.ones((internal_units,internal_units)),0,-1)\\\n",
        "                                        -tf.eye(internal_units),dtype=tf.float32)\n",
        "\n",
        "########### edge\n",
        "\n",
        "        self.max_action_edge = tf.constant(env.max_action_edge,dtype=tf.float32)        \n",
        "        self.Multiply_ones_edge = tf.tile(tf.ones((action_units_edge,action_units_edge),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_edge = tf.ones((action_units_edge,internal_units),dtype=tf.float32)\n",
        "\n",
        "############ PI controller :P\n",
        "        self.Multiply_ones_node_p = tf.tile(tf.ones((action_units_node_p,action_units_node_p),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_node_p = tf.ones((action_units_node_p,internal_units),dtype=tf.float32)\n",
        "\n",
        "############ PI controller :I\n",
        "        self.Multiply_ones_node_i = tf.tile(tf.ones((action_units_node_i,action_units_node_i),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_node_i = tf.ones((action_units_node_i,internal_units),dtype=tf.float32)\n",
        "        self.obs_zeros = tf.zeros((1, action_units_node_p))\n",
        "\n",
        "        self.linear_i = tf.zeros((action_units_node_i),dtype=np.float32)\n",
        "\n",
        "        super(MinimalRNNCell_SCNN, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "############## edge      \n",
        "        self.w_plus_temp0_edge =  self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            initializer=tf.constant_initializer(0.5),\n",
        "            trainable=True,\n",
        "            name='w_plus_temp')\n",
        "\n",
        "        self.b_plus_temp0_edge = self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            initializer=tf.keras.initializers.RandomUniform(minval=0, maxval=0.3),\n",
        "            trainable=True,\n",
        "            constraint=tf.keras.constraints.MaxNorm(0.5),\n",
        "            name='b_plus_temp')\n",
        "        self.w_minus_temp0_edge =  self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            initializer=tf.constant_initializer(0.5),\n",
        "            trainable=True,\n",
        "            name='w_minus_temp')\n",
        "\n",
        "        self.b_minus_temp0_edge = self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            initializer=tf.keras.initializers.RandomUniform(minval=0, maxval=0.3),\n",
        "            trainable=True,\n",
        "            constraint=tf.keras.constraints.MaxNorm(0.5),\n",
        "            name='b_minus_temp')\n",
        "        \n",
        "\n",
        "############# PI controller  : P\n",
        "        self.W_p1 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_p1')\n",
        "        self.b_p1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_p1')\n",
        "        self.W_p2 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_p2')\n",
        "        self.W_pz1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            constraint = tf.keras.constraints.non_neg(),\n",
        "            name='W_pz1')        \n",
        "        self.b_p2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_p2')        \n",
        "\n",
        "        self.W_p3 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,1),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_p3')\n",
        "        self.W_pz2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,1),\n",
        "            initializer='uniform',\n",
        "            trainable=True,\n",
        "            constraint = tf.keras.constraints.non_neg(),\n",
        "            name='W_pz2')           \n",
        "        self.b_p3 =  self.add_weight(\n",
        "            shape=(1,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_p3')   \n",
        "  \n",
        "        ######################### integral\n",
        "        self.W_i1 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_i1')\n",
        "        self.b_i1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_i1')\n",
        "        self.W_i2 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_i2')\n",
        "        self.W_iz1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,self.internal_units_ICNN),\n",
        "            initializer='uniform',\n",
        "            trainable=True,\n",
        "            constraint = tf.keras.constraints.non_neg(),\n",
        "            name='W_iz1')        \n",
        "        self.b_i2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_i2')        \n",
        "\n",
        "        self.W_i3 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,1),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_i3')\n",
        "        self.W_iz2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,1),\n",
        "            initializer='uniform',\n",
        "            trainable=True,\n",
        "            constraint = tf.keras.constraints.non_neg(),\n",
        "            name='W_iz2')           \n",
        "        self.b_i3 =  self.add_weight(\n",
        "            shape=(1,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_i3')   \n",
        "        self.w_beta =  self.add_weight(\n",
        "            # shape=(1),\n",
        "            initializer=tf.constant_initializer(1),\n",
        "            trainable=True,\n",
        "            name='beta')         \n",
        "        self.built = True\n",
        "\n",
        "    @tf.function    \n",
        "    def ICNN_action(self, obs):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(obs)\n",
        "            z1 = self.softplus_beta(K.dot(obs, self.W_p1)+ self.b_p1)\n",
        "            z2 = self.softplus_beta(K.dot(obs, self.W_p2)+K.dot(z1, self.W_pz1)+ self.b_p2)\n",
        "            z3 = self.softplus_beta(K.dot(obs, self.W_p3)+K.dot(z2, self.W_pz2)+ self.b_p3)\n",
        "        action_node_p = tf.squeeze(tape.batch_jacobian(z3, obs))\n",
        "\n",
        "        return action_node_p\n",
        "\n",
        "\n",
        "    @tf.function    \n",
        "    def ICNN_actionI(self, obs):\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(obs)\n",
        "            z1 = self.softplus_beta(K.dot(obs, self.W_i1)+ self.b_i1)\n",
        "            z2 = self.softplus_beta(K.dot(obs, self.W_i2)+K.dot(z1, self.W_iz1)+ self.b_i2)\n",
        "            z3 = self.softplus_beta(K.dot(obs, self.W_i3)+K.dot(z2, self.W_iz2)+ self.b_i3)\n",
        "        action_node_i = 1*tf.squeeze(tape.batch_jacobian(z3, obs))\n",
        "\n",
        "        return action_node_i\n",
        "\n",
        "    @tf.function    \n",
        "    def ICNN_edge(self, prev_state_eta):\n",
        "\n",
        "        w_plus_temp_edge = tf.math.square(self.w_plus_temp0_edge)\n",
        "        b_plus_temp_edge = tf.math.square(self.b_plus_temp0_edge)\n",
        "        w_minus_temp_edge = tf.math.square(self.w_minus_temp0_edge)\n",
        "        b_minus_temp_edge = tf.math.square(self.b_minus_temp0_edge)\n",
        "        w_plus_edge = K.dot(w_plus_temp_edge,self.w_recover)\n",
        "        b_plus_edge = K.dot(-b_plus_temp_edge,self.b_recover)\n",
        "        w_minus_edge = K.dot(-w_minus_temp_edge,self.w_recover)\n",
        "        b_minus_edge = K.dot(-b_minus_temp_edge,self.b_recover)\n",
        "\n",
        "        nonlinear_plus_edge = K.sum(K.relu(K.dot(tf.linalg.diag( prev_state_eta-self.eta0),self.ones_edge)+b_plus_edge)\\\n",
        "                        *w_plus_edge,axis=2)  \n",
        "        nonlinear_minus_edge = K.sum(K.relu(-K.dot(tf.linalg.diag(prev_state_eta-self.eta0),self.ones_edge)+b_minus_edge)\\\n",
        "                        *w_minus_edge,axis=2)  \n",
        "\n",
        "        action_nonconstrain0_edge = (nonlinear_plus_edge+nonlinear_minus_edge)\n",
        "        action_edge = self.max_action_edge - K.relu(self.max_action_edge-action_nonconstrain0_edge)\\\n",
        "                      +K.relu(-self.max_action_edge-action_nonconstrain0_edge)\n",
        "        return action_edge\n",
        "\n",
        "    @tf.function\n",
        "    def softplus_beta(self, x):\n",
        "        return K.softplus(self.w_beta*x)/(self.w_beta)\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # stacked ReLU structure to represent control network\n",
        "        prev_state = states[0]\n",
        "        prev_state_v = prev_state[:,0:self.dim_action]\n",
        "        prev_state_eta = prev_state[:, self.dim_action:self.dim_action+self.dim_edge]\n",
        "        prev_state_s =  prev_state[:,self.dim_action+self.dim_edge:self.dim_action*2+self.dim_edge]\n",
        "\n",
        "################ edge               \n",
        "        action_edge = self.ICNN_edge(prev_state_eta)\n",
        "\n",
        "##################### PI: P\n",
        "        obs_y = -prev_state_v+self.state_v_ref\n",
        "        action_node_p = self.ICNN_action(obs_y)-self.ICNN_action(self.obs_zeros)\n",
        "\n",
        "\n",
        "\n",
        "##################### PI: I\n",
        "\n",
        "\n",
        "        action_node_i = self.ICNN_actionI(prev_state_s)-self.ICNN_actionI(self.obs_zeros)\n",
        "\n",
        "#########################        \n",
        "        # calculate state on s\n",
        "        dot_s = -prev_state_v + self.state_v_ref\n",
        "\n",
        "\n",
        "#######################\n",
        "        # integrate the state transition dynamics\n",
        "\n",
        "        \n",
        "        action_network = -K.dot(action_edge,tf.transpose(self.incidence))\n",
        "        dot_v = K.dot((-prev_state_v+self.v0) + K.dot(action_network+action_node_p+action_node_i,self.diag_v1), self.diag_K)\n",
        "        dot_eta = K.dot(prev_state_v, self.incidence)\n",
        "\n",
        "\n",
        "        new_state_v = prev_state_v + self.delta_t*dot_v\n",
        "        new_state_eta = prev_state_eta + self.delta_t*dot_eta\n",
        "        new_state_s = prev_state_s + self.delta_t*dot_s\n",
        "        next_state = tf.concat([new_state_v,  new_state_eta, new_state_s], axis=1)        \n",
        "        return [new_state_v-self.state_v_ref, new_state_eta, action_network, action_node_p, action_node_i], [next_state]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E9k9PfnW0AN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loop_seed = 5\n",
        "PI_SCNN_list = []\n",
        "loss_list = []\n",
        "Comp_time_list = []\n",
        "\n",
        "\n",
        "PrintUpdate = 100\n",
        "episodes =400 # total number of iterations to update weights\n",
        "\n",
        "units = dim_action*2 + dim_edge #dimension of each state\n",
        "internal_units=20 # demension of the neural network for control policy\n",
        "internal_units_SCNN = 20\n",
        "\n",
        "T = 300  #Total period considered\n",
        "Batch_num=300 # number of batch in each episodes\n",
        "PrintUpdate=1\n",
        "ref_v_upper = 6\n",
        "ref_v_lower = 5\n",
        "\n",
        "for loop in range(1,loop_seed):\n",
        "    print('loop', loop)\n",
        "    random.seed(datetime.now())\n",
        "    start = time.time()\n",
        "\n",
        "\n",
        "    cell = MinimalRNNCell_SCNN(units, dim_edge ,dim_action, dim_action, internal_units, internal_units_SCNN,env,Batch_num)\n",
        "    layer = RNN(cell,return_sequences=True,stateful = True)\n",
        "    input_1 = tf.keras.Input(batch_shape=(Batch_num,T,units))\n",
        "    outputs = layer((input_1))\n",
        "    model_SCNN = tf.keras.models.Model([input_1], outputs)\n",
        "    model_SCNN.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "\n",
        "    x0=np.ones((Batch_num,T,units))\n",
        "    y0=model_SCNN(x0)\n",
        "    Loss_record_SCNN=[]\n",
        "    Pe_rnn_record=[]\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    learning_rate_initial=0.05\n",
        "    decayed_lr =tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        learning_rate_initial, 50, 0.7, staircase=True)\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=decayed_lr)\n",
        "\n",
        "\n",
        "    for i in range(0,episodes):\n",
        "        start_in = time.time()\n",
        "        cell.state_v_ref = tf.constant(np.random.uniform(ref_v_lower,ref_v_upper,(Batch_num, 1))@\n",
        "                                       np.ones((1,dim_action)),dtype=tf.float32)\n",
        "        initial_state1 = v0_nom*np.ones((Batch_num,dim_action)) + v0_random*np.random.uniform(0,1.,(Batch_num, dim_action))\n",
        "        initial_state2 = env.eta0*np.ones((Batch_num,dim_edge))\n",
        "        initial_state3 = np.zeros((Batch_num, dim_action))\n",
        "        initial_state = np.hstack((initial_state1, initial_state2, initial_state3))\n",
        "        layer.reset_states( initial_state)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            [new_state_v, new_state_eta, action_edge, action_P, action_I]=model_SCNN(x0)\n",
        "\n",
        "    ##################        \n",
        "            loss_state1 =1* K.sum(K.relu(-new_state_eta+1))/Batch_num/units\n",
        "            state_v_minus_ref = new_state_v\n",
        "            # -cell.state_v_ref\n",
        "            loss_state3 = K.sum(K.abs(state_v_minus_ref))/Batch_num/units\n",
        "            loss_action_edge = 0.01*K.sum(K.pow(action_edge, 2))/Batch_num/units\n",
        "            loss_action_w = 0.05*K.sum(K.pow(action_P+action_I,2)@env.diag_c)/Batch_num/units\n",
        "            loss = (loss_state3 + loss_action_edge + loss_action_w)\n",
        "\n",
        "            # loss = loss_state1 + loss_state2 + loss_state3 + loss_action_edge + loss_action_w\n",
        "\n",
        "        grads = tape.gradient(loss, model_SCNN.variables)\n",
        "        optimizer.apply_gradients(zip(grads, model_SCNN.variables))  \n",
        "        Loss_record_SCNN.append(loss)\n",
        "        if i % (PrintUpdate) == 0:\n",
        "            print('episode',i, 'Loss',loss)\n",
        "            print('   Loss_v_minus_ref',loss_state3, 'Loss_eta',loss_state1  )\n",
        "\n",
        "            print('   Loss_u_edge', loss_action_edge, 'loss_action_w', loss_action_w )\n",
        "            print('   up/ui',K.sum(K.pow(action_P,2)@env.diag_c)/K.sum(K.pow(action_I,2)@env.diag_c))\n",
        "            print('            time,',  time.time()- start_in  )\n",
        "\n",
        "    end = time.time()\n",
        "    print(end - start)   \n",
        "    Comp_time_list.append(end - start) \n",
        "    PI_SCNN_list.append(model_SCNN) \n",
        "    loss_list.append(np.array(Loss_record_SCNN)) \n",
        "\n",
        "print('computation time ', np.array(Comp_time_list))\n",
        "print('computation time mean', np.mean(np.array(Comp_time_list)))"
      ],
      "metadata": {
        "id": "kVJex-tP0AN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Loss_record_SCNN)\n",
        "plt.xlabel('episoid')\n",
        "plt.ylabel('Loss')\n"
      ],
      "metadata": {
        "id": "pk7-ZeUf0AN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear-PI"
      ],
      "metadata": {
        "id": "hoi8Mg8Ff6tb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### edge and P control\n",
        "# RNN Cell to integrate state transition dynamics\n",
        "class MinimalRNNCell_Linear(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units, action_units_edge, action_units_node_p, action_units_node_i, internal_units,internal_units_ICNN,env,batchsize,**kwargs):\n",
        "        self.units = units\n",
        "        self.action_units_edge = action_units_edge\n",
        "        self.action_units_node_p = action_units_node_p\n",
        "        self.action_units_node_i = action_units_node_i\n",
        "        self.state_size = units\n",
        "        self.internal_units = internal_units\n",
        "        self.internal_units_ICNN = internal_units_ICNN \n",
        "\n",
        "        self.batchsize=batchsize\n",
        "        self.delta_t = tf.constant(env.delta_t,dtype=tf.float32)\n",
        "\n",
        "        self.v0 = tf.constant(env.v0,dtype=tf.float32)\n",
        "        self.v1 = tf.constant(env.v1,dtype=tf.float32)\n",
        "        self.state_v_ref = tf.constant(env.state_v_ref,dtype=tf.float32)\n",
        "        self.diag_v1 = tf.constant(env.diag_v1,dtype=tf.float32)\n",
        "        self.diag_K = tf.constant(env.diag_K ,dtype=tf.float32)\n",
        "        self.delta_t = tf.constant(env.delta_t ,dtype=tf.float32)\n",
        "        self.dim_action = env.dim_action\n",
        "        self.dim_edge = env.dim_edge\n",
        "        self.eta0 = tf.constant(env.eta0 ,dtype=tf.float32)\n",
        "        self. matrix_grad_action =  tf.constant(env.matrix_grad_action,dtype=tf.float32)\n",
        "\n",
        "\n",
        "        self.Penalty_action = tf.constant(env.Penalty_action ,dtype=tf.float32)\n",
        "        self.diag_c = tf.constant(env.diag_c ,dtype=tf.float32)\n",
        "        self.incidence = tf.constant(env.incidence ,dtype=tf.float32)\n",
        "        self.w_recover =tf.constant(tf.linalg.band_part(-tf.ones((internal_units,internal_units)),0,1)\\\n",
        "                                        +2*tf.eye(internal_units),dtype=tf.float32)\n",
        "        self.b_recover = tf.constant(tf.linalg.band_part(tf.ones((internal_units,internal_units)),0,-1)\\\n",
        "                                        -tf.eye(internal_units),dtype=tf.float32)\n",
        "\n",
        "########### edge\n",
        "\n",
        "        self.max_action_edge = tf.constant(env.max_action_edge,dtype=tf.float32)        \n",
        "        self.Multiply_ones_edge = tf.tile(tf.ones((action_units_edge,action_units_edge),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_edge = tf.ones((action_units_edge,internal_units),dtype=tf.float32)\n",
        "\n",
        "############ PI controller :P\n",
        "\n",
        "        self.Multiply_ones_node_p = tf.tile(tf.ones((action_units_node_p,action_units_node_p),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_node_p = tf.ones((action_units_node_p,internal_units),dtype=tf.float32)\n",
        "\n",
        "############ PI controller :I\n",
        "        self.Multiply_ones_node_i = tf.tile(tf.ones((action_units_node_i,action_units_node_i),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_node_i = tf.ones((action_units_node_i,internal_units),dtype=tf.float32)\n",
        "        self.obs_zeros = tf.zeros((1, action_units_node_p))\n",
        "\n",
        "        self.linear_i = tf.zeros((action_units_node_i),dtype=np.float32)\n",
        "\n",
        "        super(MinimalRNNCell_Linear, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "############## edge      \n",
        "        self.k_edge =  self.add_weight(\n",
        "            shape=(self.action_units_edge,),\n",
        "            initializer=tf.keras.initializers.RandomUniform(minval=0, maxval=1),\n",
        "            trainable=True,\n",
        "            constraint=tf.keras.constraints.non_neg(),\n",
        "            name='k_edge')\n",
        "        \n",
        "\n",
        "############# PI controller  : P\n",
        "        self.W_p =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.action_units_node_p),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_p')\n",
        "        \n",
        "        ######################### integral\n",
        "        self.W_i =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.action_units_node_p),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_i')\n",
        "\n",
        "        \n",
        "        self.built = True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # stacked ReLU structure to represent control network\n",
        "        prev_state = states[0]\n",
        "        prev_state_v = prev_state[:,0:self.dim_action]\n",
        "        prev_state_eta = prev_state[:, self.dim_action:self.dim_action+self.dim_edge]\n",
        "        prev_state_s =  prev_state[:,self.dim_action+self.dim_edge:self.dim_action*2+self.dim_edge]\n",
        "\n",
        "################ edge               \n",
        "        action_edge = K.dot( prev_state_eta-self.eta0, tf.linalg.diag(self.k_edge))\n",
        "\n",
        "##################### PI: P\n",
        "        obs_y = -prev_state_v+self.state_v_ref\n",
        "        action_node_p = K.dot(obs_y, self.W_p)\n",
        "\n",
        "\n",
        "\n",
        "##################### PI: I\n",
        "\n",
        "\n",
        "        action_node_i = K.dot(prev_state_s,  self.W_i)\n",
        "\n",
        "#########################        \n",
        "        # calculate state on s\n",
        "        dot_s = -prev_state_v + self.state_v_ref\n",
        "\n",
        "\n",
        "#######################\n",
        "        # integrate the state transition dynamics\n",
        "\n",
        "        \n",
        "        action_network = -K.dot(action_edge,tf.transpose(self.incidence))\n",
        "        dot_v = K.dot((-prev_state_v+self.v0) + K.dot(action_network+action_node_p+action_node_i,self.diag_v1), self.diag_K)\n",
        "        dot_eta = K.dot(prev_state_v, self.incidence)\n",
        "\n",
        "\n",
        "        new_state_v = prev_state_v + self.delta_t*dot_v\n",
        "        new_state_eta = prev_state_eta + self.delta_t*dot_eta\n",
        "        new_state_s = prev_state_s + self.delta_t*dot_s\n",
        "\n",
        "        next_state = tf.concat([new_state_v,  new_state_eta, new_state_s], axis=1)        \n",
        "        return [new_state_v, new_state_eta, action_network, action_node_p, action_node_i], [next_state]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fJKXzc84f6tb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loop_seed = 5\n",
        "PI_Linear_list = []\n",
        "loss_list = []\n",
        "Comp_time_list = []\n",
        "\n",
        "\n",
        "PrintUpdate = 100\n",
        "episodes =400 # total number of iterations to update weights\n",
        "\n",
        "\n",
        "units = dim_action*2 + dim_edge #dimension of each state\n",
        "internal_units=20 # demension of the neural network for control policy\n",
        "internal_units_Linear = 20\n",
        "\n",
        "T = 300  #Total period considered\n",
        "Batch_num=300 # number of batch in each episodes\n",
        "PrintUpdate=1\n",
        "\n",
        "for loop in range(0,loop_seed):\n",
        "    random.seed(datetime.now())\n",
        "    start = time.time()\n",
        "\n",
        "\n",
        "    cell = MinimalRNNCell_Linear(units, dim_edge ,dim_action, dim_action, internal_units, internal_units_Linear,env,Batch_num)\n",
        "    layer = RNN(cell,return_sequences=True,stateful = True)\n",
        "    input_1 = tf.keras.Input(batch_shape=(Batch_num,T,units))\n",
        "    outputs = layer((input_1))\n",
        "    model_Linear = tf.keras.models.Model([input_1], outputs)\n",
        "    model_Linear.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "\n",
        "    x0=np.ones((Batch_num,T,units))\n",
        "    y0=model_Linear(x0)\n",
        "    Loss_record_Linear=[]\n",
        "    Pe_rnn_record=[]\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    learning_rate_initial=0.05\n",
        "    decayed_lr =tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        learning_rate_initial, 50, 0.7, staircase=True)\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=decayed_lr)\n",
        "\n",
        "\n",
        "    for i in range(0,episodes):\n",
        "        start_in = time.time()\n",
        "\n",
        "        initial_state1 = v0_nom*np.ones((Batch_num,dim_action)) + v0_random*np.random.uniform(0,1.,(Batch_num, dim_action))\n",
        "        initial_state2 = env.eta0*np.ones((Batch_num,dim_edge))\n",
        "        initial_state3 = np.zeros((Batch_num, dim_action))\n",
        "        initial_state = np.hstack((initial_state1, initial_state2, initial_state3))\n",
        "        layer.reset_states( initial_state)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            [new_state_v, new_state_eta, action_edge, action_P, action_I]=model_Linear(x0)\n",
        "\n",
        "\n",
        "    ##################        \n",
        "            loss_state1 =1* K.sum(K.relu(-new_state_eta+1))/Batch_num/units\n",
        "\n",
        "            state_v_minus_ref = new_state_v-env.state_v_ref\n",
        "            loss_state3 = K.sum(K.abs(state_v_minus_ref))/Batch_num/units\n",
        "            loss_action_edge = 0.01*K.sum(K.pow(action_edge, 2))/Batch_num/units\n",
        "            loss_action_w = 0.05*K.sum(K.pow(action_P+action_I,2)@env.diag_c)/Batch_num/units\n",
        "            loss = (loss_state3 + loss_action_edge + loss_action_w)\n",
        "\n",
        "\n",
        "        grads = tape.gradient(loss, model_Linear.variables)\n",
        "        optimizer.apply_gradients(zip(grads, model_Linear.variables))  \n",
        "        Loss_record_Linear.append(loss)\n",
        "        if i % (PrintUpdate) == 0:\n",
        "            print('episode',i, 'Loss',loss)\n",
        "            print('   Loss_v_minus_ref',loss_state3, 'Loss_eta',loss_state1  )\n",
        "            print('   Loss_u_edge', loss_action_edge, 'loss_action_w', loss_action_w )\n",
        "            print('   up/ui',K.sum(K.pow(action_P,2)@env.diag_c)/K.sum(K.pow(action_I,2)@env.diag_c))\n",
        "            print('            time,',  time.time()- start_in  )\n",
        "\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    print(end - start)   \n",
        "    Comp_time_list.append(end - start) \n",
        "    PI_Linear_list.append(model_Linear) \n",
        "    loss_list.append(np.array(Loss_record_Linear)) \n",
        "\n",
        "\n",
        "\n",
        "print('computation time ', np.array(Comp_time_list))\n",
        "print('computation time mean', np.mean(np.array(Comp_time_list)))"
      ],
      "metadata": {
        "id": "VR0LKz85f6tc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Loss_record_Linear)\n",
        "plt.xlabel('episoid')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Non-Discounted Loss without penalty')"
      ],
      "metadata": {
        "id": "ithDjXjYf6td"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNN-PI"
      ],
      "metadata": {
        "id": "yUMYv92c78Mw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### edge and P control\n",
        "# RNN Cell to integrate state transition dynamics\n",
        "class MinimalRNNCell_NNMatrix(keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units, action_units_edge, action_units_node_p, action_units_node_i, internal_units,internal_units_ICNN,env,batchsize,**kwargs):\n",
        "        self.units = units\n",
        "        self.action_units_edge = action_units_edge\n",
        "        self.action_units_node_p = action_units_node_p\n",
        "        self.action_units_node_i = action_units_node_i\n",
        "        self.state_size = units\n",
        "        self.internal_units = internal_units\n",
        "        self.internal_units_ICNN = internal_units_ICNN \n",
        "\n",
        "        self.batchsize=batchsize\n",
        "        self.delta_t = tf.constant(env.delta_t,dtype=tf.float32)\n",
        "\n",
        "        self.v0 = tf.constant(env.v0,dtype=tf.float32)\n",
        "        self.v1 = tf.constant(env.v1,dtype=tf.float32)\n",
        "        self.state_v_ref = tf.constant(env.state_v_ref*np.ones((batchsize, action_units_node_p)),dtype=tf.float32)\n",
        "        self.diag_v1 = tf.constant(env.diag_v1,dtype=tf.float32)\n",
        "        self.diag_K = tf.constant(env.diag_K ,dtype=tf.float32)\n",
        "        self.delta_t = tf.constant(env.delta_t ,dtype=tf.float32)\n",
        "        self.dim_action = env.dim_action\n",
        "        # tf.constant( ,dtype=tf.float32)\n",
        "        self.dim_edge = env.dim_edge\n",
        "        self.eta0 = tf.constant(env.eta0 ,dtype=tf.float32)\n",
        "        self. matrix_grad_action =  tf.constant(env.matrix_grad_action,dtype=tf.float32)\n",
        "\n",
        "\n",
        "        self.Penalty_action = tf.constant(env.Penalty_action ,dtype=tf.float32)\n",
        "        self.diag_c = tf.constant(env.diag_c ,dtype=tf.float32)\n",
        "        self.incidence = tf.constant(env.incidence ,dtype=tf.float32)\n",
        "        self.w_recover =tf.constant(tf.linalg.band_part(-tf.ones((internal_units,internal_units)),0,1)\\\n",
        "                                        +2*tf.eye(internal_units),dtype=tf.float32)\n",
        "        self.b_recover = tf.constant(tf.linalg.band_part(tf.ones((internal_units,internal_units)),0,-1)\\\n",
        "                                        -tf.eye(internal_units),dtype=tf.float32)\n",
        "\n",
        "########### edge\n",
        "\n",
        "        self.max_action_edge = tf.constant(env.max_action_edge,dtype=tf.float32)        \n",
        "        self.Multiply_ones_edge = tf.tile(tf.ones((action_units_edge,action_units_edge),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_edge = tf.ones((action_units_edge,internal_units),dtype=tf.float32)\n",
        "\n",
        "############ PI controller :P\n",
        "\n",
        "        self.Multiply_ones_node_p = tf.tile(tf.ones((action_units_node_p,action_units_node_p),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_node_p = tf.ones((action_units_node_p,internal_units),dtype=tf.float32)\n",
        "\n",
        "############ PI controller :I\n",
        "        self.Multiply_ones_node_i = tf.tile(tf.ones((action_units_node_i,action_units_node_i),dtype=np.float32)[None], [batchsize, 1, 1]) \n",
        "        self.ones_node_i = tf.ones((action_units_node_i,internal_units),dtype=tf.float32)\n",
        "        self.obs_zeros = tf.zeros((1, action_units_node_p))\n",
        "\n",
        "        self.linear_i = tf.zeros((action_units_node_i),dtype=np.float32)\n",
        "\n",
        "        super(MinimalRNNCell_NNMatrix, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "############## edge      \n",
        "        self.w_plus_temp0_edge =  self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            # initializer=tf.constant_initializer(0.5),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='w_plus_temp')\n",
        "\n",
        "        self.b_plus_temp0_edge = self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_plus_temp')\n",
        "        self.w_minus_temp0_edge =  self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='w_minus_temp')\n",
        "\n",
        "        self.b_minus_temp0_edge = self.add_weight(\n",
        "            shape=(self.action_units_edge,self.internal_units),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_minus_temp')\n",
        "        \n",
        "\n",
        "############# PI controller  : P\n",
        "        self.W_p1 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_p1')\n",
        "        self.b_p1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_p1')\n",
        "        self.W_p2 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_p2')\n",
        "        self.W_pz1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_pz1')        \n",
        "        self.b_p2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_p2')        \n",
        "\n",
        "        self.W_p3 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,1),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_p3')\n",
        "        self.W_pz2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,self.action_units_node_p),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_pz2')           \n",
        "        self.b_p3 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_p3')   \n",
        "        \n",
        "        ######################### integral\n",
        "        self.W_i1 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_i1')\n",
        "        self.b_i1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_i1')\n",
        "        self.W_i2 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_i2')\n",
        "        self.W_iz1 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,self.internal_units_ICNN),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_iz1')        \n",
        "        self.b_i2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_i2')        \n",
        "\n",
        "        self.W_i3 =  self.add_weight(\n",
        "            shape=(self.action_units_node_p,1),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_i3')\n",
        "        self.W_iz2 =  self.add_weight(\n",
        "            shape=(self.internal_units_ICNN,self.action_units_node_i),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='W_iz2')           \n",
        "        self.b_i3 =  self.add_weight(\n",
        "            shape=(self.action_units_node_i,),\n",
        "            initializer='random_normal',\n",
        "            trainable=True,\n",
        "            name='b_i3')   \n",
        "\n",
        "        \n",
        "        self.built = True\n",
        "\n",
        "    @tf.function    \n",
        "    def ICNN_action(self, obs):\n",
        "\n",
        "        z1 = K.softplus(K.dot(obs, self.W_p1)+ self.b_p1)\n",
        "        z2 = K.softplus(K.dot(obs, self.W_p2)+K.dot(z1, self.W_pz1)+ self.b_p2)\n",
        "        z3 = K.dot(obs, self.W_p3)+K.dot(z2, self.W_pz2)+ self.b_p3\n",
        "        return z3\n",
        "\n",
        "    @tf.function    \n",
        "    def ICNN_actionI(self, obs):\n",
        "\n",
        "        z1 = K.softplus(K.dot(obs, self.W_i1)+ self.b_i1)\n",
        "        z2 = K.softplus(K.dot(obs, self.W_i2)+K.dot(z1, self.W_iz1)+ self.b_i2)\n",
        "        z3 = K.dot(obs, self.W_i3)+K.dot(z2, self.W_iz2)+ self.b_i3\n",
        "        return z3\n",
        "\n",
        "    @tf.function    \n",
        "    def ICNN_edge(self, prev_state_eta):\n",
        "        action_nonconstrain0_edge = K.sum(K.relu(K.dot(tf.linalg.diag(prev_state_eta-self.eta0),self.w_plus_temp0_edge)\\\n",
        "                                         +self.b_plus_temp0_edge)*self.w_minus_temp0_edge+self.b_minus_temp0_edge,axis=2)\n",
        "        action_edge = self.max_action_edge - K.relu(self.max_action_edge-action_nonconstrain0_edge)\\\n",
        "                      +K.relu(-self.max_action_edge-action_nonconstrain0_edge)\n",
        "        return action_edge\n",
        "\n",
        "\n",
        "    def call(self, inputs, states):\n",
        "        # stacked ReLU structure to represent control network\n",
        "        prev_state = states[0]\n",
        "        prev_state_v = prev_state[:,0:self.dim_action]\n",
        "        prev_state_eta = prev_state[:, self.dim_action:self.dim_action+self.dim_edge]\n",
        "        prev_state_s =  prev_state[:,self.dim_action+self.dim_edge:self.dim_action*2+self.dim_edge]\n",
        "\n",
        "################ edge               \n",
        "        action_edge = self.ICNN_edge(prev_state_eta)\n",
        "\n",
        "##################### PI: P\n",
        "        obs_y = -prev_state_v+self.state_v_ref\n",
        "        action_node_p = self.ICNN_action(obs_y)\n",
        "\n",
        "\n",
        "\n",
        "##################### PI: I\n",
        "\n",
        "\n",
        "        action_node_i = self.ICNN_actionI(prev_state_s)\n",
        "\n",
        "#########################        \n",
        "        # calculate state on s\n",
        "        dot_s = -prev_state_v + self.state_v_ref\n",
        "\n",
        "\n",
        "#######################\n",
        "        # integrate the state transition dynamics\n",
        "\n",
        "        \n",
        "        action_network = -K.dot(action_edge,tf.transpose(self.incidence))\n",
        "        dot_v = K.dot((-prev_state_v+self.v0) + K.dot(action_network+action_node_p+action_node_i,self.diag_v1), self.diag_K)\n",
        "        dot_eta = K.dot(prev_state_v, self.incidence)\n",
        "\n",
        "\n",
        "        new_state_v = prev_state_v + self.delta_t*dot_v\n",
        "        new_state_eta = prev_state_eta + self.delta_t*dot_eta\n",
        "        new_state_s = prev_state_s + self.delta_t*dot_s\n",
        "\n",
        "        next_state = tf.concat([new_state_v,  new_state_eta, new_state_s], axis=1)        \n",
        "        return [new_state_v-self.state_v_ref, new_state_eta, action_network, action_node_p, action_node_i], [next_state]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WhVxTo3W78Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loop_seed = 5\n",
        "PI_NNMatrix_list = []\n",
        "loss_list = []\n",
        "loss_all_list = []\n",
        "\n",
        "Comp_time_list = []\n",
        "\n",
        "\n",
        "episodes =400 # total number of iterations to update weights\n",
        "# action_units = dim_edge\n",
        "# action_units_edge, action_units_node_p, action_units_node_i\n",
        "\n",
        "units = dim_action*2 + dim_edge #dimension of each state\n",
        "internal_units=20 # demension of the neural network for control policy\n",
        "internal_units_NNMatrix = 20\n",
        "\n",
        "T = 300  #Total period considered\n",
        "Batch_num=300 # number of batch in each episodes\n",
        "PrintUpdate=1\n",
        "ref_v_upper = 6\n",
        "ref_v_lower = 5\n",
        "\n",
        "for loop in range(0,loop_seed):\n",
        "    print('loop', loop)\n",
        "    random.seed(datetime.now())\n",
        "    start = time.time()\n",
        "\n",
        "\n",
        "    cell = MinimalRNNCell_NNMatrix(units, dim_edge ,dim_action, dim_action, internal_units, internal_units_NNMatrix,env,Batch_num)\n",
        "    layer = RNN(cell,return_sequences=True,stateful = True)\n",
        "    input_1 = tf.keras.Input(batch_shape=(Batch_num,T,units))\n",
        "    outputs = layer((input_1))\n",
        "    model_NNMatrix = tf.keras.models.Model([input_1], outputs)\n",
        "    model_NNMatrix.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
        "\n",
        "    x0=np.ones((Batch_num,T,units))\n",
        "    y0=model_NNMatrix(x0)\n",
        "    Loss_record_NNMatrix=[]\n",
        "    Loss_all_record_NNMatrix=[]\n",
        "    global_step = tf.Variable(0, trainable=False)\n",
        "    learning_rate_initial=0.035\n",
        "    decayed_lr =tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        learning_rate_initial, 50, 0.7, staircase=True)\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=decayed_lr)\n",
        "\n",
        "\n",
        "    for i in range(0,episodes):\n",
        "        start_in = time.time()\n",
        "        cell.state_v_ref = tf.constant(np.random.uniform(ref_v_lower,ref_v_upper,(Batch_num, 1))@\n",
        "                                       np.ones((1,dim_action)),dtype=tf.float32)\n",
        "        initial_state1 = v0_nom*np.ones((Batch_num,dim_action)) + v0_random*np.random.uniform(0,1.,(Batch_num, dim_action))\n",
        "        initial_state2 = env.eta0*np.ones((Batch_num,dim_edge))\n",
        "        initial_state3 = np.zeros((Batch_num, dim_action))\n",
        "        initial_state = np.hstack((initial_state1, initial_state2, initial_state3))\n",
        "        layer.reset_states( initial_state)\n",
        "        with tf.GradientTape(persistent=True) as tape:\n",
        "            [new_state_v, new_state_eta, action_edge, action_P, action_I]=model_NNMatrix(x0)\n",
        "\n",
        "    ##################        \n",
        "            loss_state1 =1* K.sum(K.relu(-new_state_eta+1))/Batch_num/units\n",
        "\n",
        "            state_v_minus_ref = new_state_v\n",
        "            loss_state3 = K.sum(K.abs(state_v_minus_ref))/Batch_num/units\n",
        "            loss_action_edge = 0.01*K.sum(K.pow(action_edge, 2))/Batch_num/units\n",
        "            loss_action_w = 0.05*K.sum(K.pow(action_P+action_I,2)@env.diag_c)/Batch_num/units\n",
        "            loss = (loss_state3 + loss_action_edge + loss_action_w)\n",
        "            loss_state_large = 10*K.sum(K.relu(K.abs(state_v_minus_ref)-5))/Batch_num/units\n",
        "            loss_all = loss + loss_state_large\n",
        "\n",
        "        grads = tape.gradient(loss, model_NNMatrix.variables)\n",
        "        optimizer.apply_gradients(zip(grads, model_NNMatrix.variables))  \n",
        "        Loss_record_NNMatrix.append(loss)\n",
        "        Loss_all_record_NNMatrix.append(loss_all)\n",
        "        if i % (PrintUpdate) == 0:\n",
        "            print('episode',i, 'loss_all',loss_all, 'Loss',loss, 'loss_state_large', loss_state_large)\n",
        "            print('   Loss_v_minus_ref',loss_state3, 'Loss_eta',loss_state1  )\n",
        "            print('   Loss_u_edge', loss_action_edge, 'loss_action_w', loss_action_w )\n",
        "            print('   up/ui',K.sum(K.pow(action_P,2)@env.diag_c)/K.sum(K.pow(action_I,2)@env.diag_c))\n",
        "            print('            time,',  time.time()- start_in  )\n",
        "\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    print(end - start)   \n",
        "    Comp_time_list.append(end - start) \n",
        "    PI_NNMatrix_list.append(model_NNMatrix) \n",
        "    loss_list.append(np.array(Loss_record_NNMatrix)) \n",
        "    loss_all_list.append(np.array(Loss_all_record_NNMatrix)) \n",
        "\n",
        "\n",
        "\n",
        "print('computation time ', np.array(Comp_time_list))\n",
        "print('computation time mean', np.mean(np.array(Comp_time_list)))    \n"
      ],
      "metadata": {
        "id": "ns46lxng78Mx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(Loss_record_NNMatrix)\n",
        "plt.xlabel('episoid')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Non-Discounted Loss without penalty')"
      ],
      "metadata": {
        "id": "9DBpijgu78My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "okJiY-38RSXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simulate "
      ],
      "metadata": {
        "id": "IOTnNnZtHG4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural-PI"
      ],
      "metadata": {
        "id": "60P8nworSBLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def Action_edge(state, model, env):\n",
        "    \n",
        "    w_plus=K.dot(tf.math.square(model.variables[0]),model.w_recover)\n",
        "    b_plus=K.dot(-tf.math.square(model.variables[1]),model.b_recover)\n",
        "    w_minus=K.dot(-tf.math.square(model.variables[2]),model.w_recover)\n",
        "    b_minus=K.dot(-tf.math.square(model.variables[3]),model.b_recover)\n",
        "    nonlinear_plus=K.sum(K.relu(K.dot(tf.linalg.diag(state-env.eta0),model.ones_edge)+b_plus)\\\n",
        "                    *w_plus,axis=2)  \n",
        "    nonlinear_minus=K.sum(K.relu(-K.dot(tf.linalg.diag(state-env.eta0),model.ones_edge)+b_minus)\\\n",
        "                    *w_minus,axis=2)  \n",
        "    action_nonconstrain0= nonlinear_plus+nonlinear_minus\n",
        "    action=env.max_action_edge-np_relu(env.max_action_edge-action_nonconstrain0)\\\n",
        "            +np_relu(-env.max_action_edge-action_nonconstrain0)\n",
        "\n",
        "    return action\n",
        "\n",
        "@tf.function\n",
        "def Action_ICNN_P(obs, model, env):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(obs)\n",
        "        z1 = model.softplus_beta(K.dot(obs, model.variables[4])+ model.variables[5])\n",
        "        z2 = model.softplus_beta(K.dot(obs, model.variables[6])+K.dot(z1, model.variables[7])+ model.variables[8])\n",
        "        z3 = model.softplus_beta(K.dot(obs, model.variables[9])+K.dot(z2, model.variables[10])+ model.variables[11])\n",
        "  \n",
        "    return tf.squeeze(tape.batch_jacobian(z3, obs))\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def Action_ICNN_I(obs, model, env):\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(obs)\n",
        "        z1 = model.softplus_beta(K.dot(obs, model.variables[12])+ model.variables[13])\n",
        "        z2 = model.softplus_beta(K.dot(obs, model.variables[14])+K.dot(z1, model.variables[15])+ model.variables[16])\n",
        "        z3 = model.softplus_beta(K.dot(obs, model.variables[17])+K.dot(z2, model.variables[18])+ model.variables[19])\n",
        "  \n",
        "    return tf.squeeze(tape.batch_jacobian(z3, obs), axis=0)\n",
        "\n",
        "\n",
        "#\n",
        "###########    add\n",
        "def Action_ICNN(state_x, state_s, model, env):\n",
        "  \n",
        "    action_nonconstrain0 = Action_ICNN_P(-state_x+env.state_v_ref, model, env)\\\n",
        "                          -Action_ICNN_P(model.obs_zeros, model, env)\n",
        "    action_nonconstrain1 = Action_ICNN_I(state_s,model, env)-Action_ICNN_I(model.obs_zeros, model, env)\n",
        "    action_nonconstrain =  action_nonconstrain0 + action_nonconstrain1\n",
        "    return action_nonconstrain, action_nonconstrain0, action_nonconstrain1\n"
      ],
      "metadata": {
        "id": "t1QEQqXhsZy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7_9LYIF7sZy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the trajectory to visulize the performance of control\n",
        "Trajectory_Linear = [] \n",
        "Trajectory_eta_Linear = [] \n",
        "Trajectory_s_Linear = [] \n",
        "\n",
        "init_state = np.array([[4.5481234, 4.4400673, 3.0106626, 4.648548 , 3.862493 , 3.4021516,\n",
        "        4.1023088, 6.730633 , 4.2614346, 5.3563185, 5.5848107, 5.681905 ,\n",
        "        3.5720792, 4.010938 , 6.539898 , 3.4076588, 6.5165396, 4.065842 ,\n",
        "        6.97986  , 5.6366544 ]], dtype=np.float32)\n",
        "# (v0_nom*np.ones(dim_action,dtype=np.float32) + \n",
        "              # v0_random*np.random.uniform(0,1.,(dim_action)).astype(np.float32)).reshape((1, -1))\n",
        "\n",
        "# v0.reshape((1, -1))\n",
        "linear_coff_s = np.ones((1,dim_action),dtype=np.float32)*(2)\n",
        "linear_coff_x = linear_coff= np.ones((1,dim_action),dtype=np.float32)*10\n",
        "\n",
        "x = init_state\n",
        "action_s = np.zeros((1,dim_action),dtype=np.float32)\n",
        "\n",
        "env.set_state(x)\n",
        "\n",
        "Trajectory_Linear.append(x)\n",
        "Trajectory_eta_Linear.append(env.state_eta)\n",
        "\n",
        "SimulationLength=760\n",
        "Record_u_Linear=[]\n",
        "Record_Loss_Linear=[]\n",
        "Record_action_network_Linear=[]\n",
        "Loss_Linear=0\n",
        "state_s = np.zeros((1,dim_action),dtype=np.float32)\n",
        "Record_up=[]\n",
        "Record_ui=[]\n",
        "record_grad_ui = []\n",
        "Trajectory_s_Linear.append(env.state_s)\n",
        "\n",
        "for i in range(SimulationLength):\n",
        "\n",
        "\n",
        "    action_edgefeedback = Action_edge(env.state_eta, PI_SCNN_list[0], env)\n",
        "\n",
        "    u, up, ui = Action_ICNN(x, state_s, PI_SCNN_list[0], env)\n",
        "    next_state_s, next_state_v, next_state_eta, r, action_network= env.step_edge_WoCost(action_edgefeedback, up, ui)\n",
        "\n",
        "    Loss_Linear+=r\n",
        "    x = next_state_v\n",
        "    state_s = next_state_s\n",
        "    Trajectory_Linear.append(x)\n",
        "    Trajectory_eta_Linear.append(next_state_eta)\n",
        "    Trajectory_s_Linear.append(next_state_s)\n",
        "    Record_u_Linear.append(np.squeeze(u))\n",
        "    Record_up.append(np.squeeze(up))\n",
        "    Record_ui.append(np.squeeze(ui))\n",
        "    Record_action_network_Linear.append(np.squeeze(action_network))\n",
        "    Record_Loss_Linear.append(np.squeeze(r))\n",
        "    record_grad_ui.append(np.squeeze(env.calc_grad_action(ui)[1]))\n",
        "\n",
        "Trajectory_Linear = np.squeeze(np.asarray(Trajectory_Linear))\n",
        "Trajectory_s_Linear = np.squeeze(np.asarray(Trajectory_s_Linear))\n",
        "record_grad_ui = np.squeeze(np.asarray(record_grad_ui))\n",
        "Record_u_Linear = np.squeeze(np.asarray(Record_u_Linear))\n",
        "fig = plt.figure(figsize=(11,10), dpi=100)\n",
        "\n",
        "plt.subplot(4,2,1)\n",
        "\n",
        "\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_u_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action_total')\n",
        "\n",
        "plt.subplot(4,2,2)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_action_network_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action_network')\n",
        "\n",
        "plt.subplot(4,2,3)\n",
        "\n",
        "TimeRecord=np.arange(1,SimulationLength+2)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Trajectory_Linear)\n",
        "\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('speed')\n",
        "\n",
        "Trajectory_eta_Linear=np.squeeze(np.asarray(Trajectory_eta_Linear))\n",
        "\n",
        "plt.subplot(4,2,4)\n",
        "plt.plot(TimeRecord,Trajectory_eta_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('position')\n",
        "\n",
        "plt.subplot(4,2,5)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_up)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action u_p')\n",
        "\n",
        "\n",
        "plt.subplot(4,2,6)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_ui)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action u_i')\n",
        "fig.tight_layout()   \n",
        "\n",
        "plt.subplot(4,2,7)\n",
        "TimeRecord=np.arange(1,SimulationLength+2)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Trajectory_s_Linear)\n",
        "\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('state_s')\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(4,2,8)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,record_grad_ui)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('grad u_i')\n",
        "fig.tight_layout()   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R-B6h1RhJ9N-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear-PI"
      ],
      "metadata": {
        "id": "yLGgwDHfSEOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def Action_Linear_edge(state, model, env):\n",
        "    action_nonconstrain0 = (state-env.eta0)*abs(model.variables[0])\n",
        "    action=env.max_action_edge-np_relu(env.max_action_edge-action_nonconstrain0)\\\n",
        "            +np_relu(-env.max_action_edge-action_nonconstrain0)\n",
        "    return action\n",
        "\n",
        "@tf.function\n",
        "def Action_Linear_P(obs, model, env):\n",
        "    z1 = K.dot(obs,  model.variables[1])\n",
        "    return z1\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def Action_Linear_I(obs, model, env):\n",
        "    z1 = K.dot(obs,  model.variables[2])\n",
        "    return z1\n",
        "\n",
        "###########    add\n",
        "def Action_Linear(state_x, state_s, model, env):\n",
        "  \n",
        "    action_nonconstrain0 = Action_Linear_P(-state_x+env.state_v_ref, model, env)\n",
        "    action_nonconstrain1 = Action_Linear_I(state_s,model, env)\n",
        "    action_nonconstrain =  action_nonconstrain0 + action_nonconstrain1\n",
        "\n",
        "    return action_nonconstrain, action_nonconstrain0, action_nonconstrain1\n"
      ],
      "metadata": {
        "id": "jPw22L6vSGSb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the trajectory to visulize the performance of control\n",
        "\n",
        "Trajectory_Linear = [] \n",
        "Trajectory_eta_Linear = [] \n",
        "Trajectory_s_Linear = [] \n",
        "\n",
        "init_state = np.array([[4.5481234, 4.4400673, 3.0106626, 4.648548 , 3.862493 , 3.4021516,\n",
        "        4.1023088, 6.730633 , 4.2614346, 5.3563185, 5.5848107, 5.681905 ,\n",
        "        3.5720792, 4.010938 , 6.539898 , 3.4076588, 6.5165396, 4.065842 ,\n",
        "        6.97986  , 5.6366544 ]], dtype=np.float32)\n",
        "# (v0_nom*np.ones(dim_action,dtype=np.float32) + \n",
        "              # v0_random*np.random.uniform(0,1.,(dim_action)).astype(np.float32)).reshape((1, -1))\n",
        "\n",
        "# v0.reshape((1, -1))\n",
        "linear_coff_s = np.ones((1,dim_action),dtype=np.float32)*(2)\n",
        "linear_coff_x = linear_coff= np.ones((1,dim_action),dtype=np.float32)*10\n",
        "\n",
        "x = init_state\n",
        "action_s = np.zeros((1,dim_action),dtype=np.float32)\n",
        "\n",
        "env.set_state(x)\n",
        "\n",
        "Trajectory_Linear.append(x)\n",
        "Trajectory_eta_Linear.append(env.state_eta)\n",
        "\n",
        "SimulationLength=760\n",
        "Record_u_Linear=[]\n",
        "Record_Loss_Linear=[]\n",
        "Record_action_network_Linear=[]\n",
        "Loss_Linear=0\n",
        "state_s = np.zeros((1,dim_action),dtype=np.float32)\n",
        "Record_up=[]\n",
        "Record_ui=[]\n",
        "record_grad_ui = []\n",
        "Trajectory_s_Linear.append(env.state_s)\n",
        "\n",
        "for i in range(SimulationLength):\n",
        "\n",
        "    id_model =3\n",
        "\n",
        "    action_edgefeedback = Action_Linear_edge(env.state_eta,  PI_Linear_list[id_model], env)\n",
        "\n",
        "    u, up, ui = Action_Linear(x, state_s,  PI_Linear_list[id_model], env)\n",
        "\n",
        "    next_state_s, next_state_v, next_state_eta, r, action_network= env.step_edge_WoCost(action_edgefeedback, up, ui)\n",
        "\n",
        "    Loss_Linear+=r\n",
        "    x = next_state_v\n",
        "    state_s = next_state_s\n",
        "    Trajectory_Linear.append(x)\n",
        "    Trajectory_eta_Linear.append(next_state_eta)\n",
        "    Trajectory_s_Linear.append(next_state_s)\n",
        "    Record_u_Linear.append(np.squeeze(u))\n",
        "    Record_up.append(np.squeeze(up))\n",
        "    Record_ui.append(np.squeeze(ui))\n",
        "    Record_action_network_Linear.append(np.squeeze(action_network))\n",
        "    Record_Loss_Linear.append(np.squeeze(r))\n",
        "    record_grad_ui.append(np.squeeze(env.calc_grad_action(ui)[1]))\n",
        "\n",
        "Trajectory_Linear = np.squeeze(np.asarray(Trajectory_Linear))\n",
        "Trajectory_s_Linear = np.squeeze(np.asarray(Trajectory_s_Linear))\n",
        "record_grad_ui = np.squeeze(np.asarray(record_grad_ui))\n",
        "Record_u_Linear = np.squeeze(np.asarray(Record_u_Linear))\n",
        "fig = plt.figure(figsize=(11,10), dpi=100)\n",
        "\n",
        "plt.subplot(4,2,1)\n",
        "\n",
        "\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_u_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action_total')\n",
        "\n",
        "plt.subplot(4,2,2)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_action_network_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action_network')\n",
        "\n",
        "plt.subplot(4,2,3)\n",
        "\n",
        "TimeRecord=np.arange(1,SimulationLength+2)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Trajectory_Linear)\n",
        "\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('speed')\n",
        "\n",
        "Trajectory_eta_Linear=np.squeeze(np.asarray(Trajectory_eta_Linear))\n",
        "\n",
        "plt.subplot(4,2,4)\n",
        "plt.plot(TimeRecord,Trajectory_eta_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('position')\n",
        "\n",
        "plt.subplot(4,2,5)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_up)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action u_p')\n",
        "\n",
        "\n",
        "plt.subplot(4,2,6)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_ui)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action u_i')\n",
        "fig.tight_layout()   \n",
        "\n",
        "plt.subplot(4,2,7)\n",
        "TimeRecord=np.arange(1,SimulationLength+2)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Trajectory_s_Linear)\n",
        "\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('state_s')\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(4,2,8)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,record_grad_ui)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('grad u_i')\n",
        "fig.tight_layout()   \n"
      ],
      "metadata": {
        "id": "gsRSAk3DfeA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DenseNN-PI"
      ],
      "metadata": {
        "id": "Fev-P4-kTAM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @tf.function\n",
        "def Action_edge(state, model, env):\n",
        "    action_nonconstrain0= K.sum(K.relu(K.dot(tf.linalg.diag(state-env.eta0),model.variables[0])+model.variables[1])*\\\n",
        "                  model.variables[2]+model.variables[3],axis=2)\n",
        "    action=env.max_action_edge-np_relu(env.max_action_edge-action_nonconstrain0)\\\n",
        "            +np_relu(-env.max_action_edge-action_nonconstrain0)\n",
        "    return action\n",
        "\n",
        "@tf.function\n",
        "def Action_NNMatrix_P(obs, model, env):\n",
        "    z1 = K.softplus(K.dot(obs, model.variables[4])+ model.variables[5])\n",
        "    z2 = K.softplus(K.dot(obs, model.variables[6])+K.dot(z1, model.variables[7])+ model.variables[8])\n",
        "    z3 = K.dot(obs, model.variables[9])+K.dot(z2, model.variables[10])+ model.variables[11]\n",
        "    return z3\n",
        "\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def Action_NNMatrix_I(obs, model, env):\n",
        "\n",
        "    z1 = K.softplus(K.dot(obs, model.variables[12])+ model.variables[13])\n",
        "    z2 = K.softplus(K.dot(obs, model.variables[14])+K.dot(z1, model.variables[15])+ model.variables[16])\n",
        "    z3 = K.dot(obs, model.variables[17])+K.dot(z2, model.variables[18])+ model.variables[19]\n",
        "    return z3\n",
        "\n",
        "#\n",
        "###########    add\n",
        "def Action_NNMatrix(state_x, state_s, model, env):\n",
        "  \n",
        "    action_nonconstrain0 = Action_NNMatrix_P(-state_x+env.state_v_ref, model, env)\n",
        "    action_nonconstrain1 = Action_NNMatrix_I(state_s,model, env)\n",
        "    action_nonconstrain =  action_nonconstrain0 + action_nonconstrain1\n",
        "    return action_nonconstrain, action_nonconstrain0, action_nonconstrain1\n"
      ],
      "metadata": {
        "id": "9808H46KTE-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the trajectory to visulize the performance of control\n",
        "\n",
        "Trajectory_Linear = [] \n",
        "Trajectory_eta_Linear = [] \n",
        "Trajectory_s_Linear = [] \n",
        "\n",
        "init_state = np.array([[4.5481234, 4.4400673, 3.0106626, 4.648548 , 3.862493 , 3.4021516,\n",
        "        4.1023088, 6.730633 , 4.2614346, 5.3563185, 5.5848107, 5.681905 ,\n",
        "        3.5720792, 4.010938 , 6.539898 , 3.4076588, 6.5165396, 4.065842 ,\n",
        "        6.97986  , 5.6366544 ]], dtype=np.float32)\n",
        "# (v0_nom*np.ones(dim_action,dtype=np.float32) + \n",
        "              # v0_random*np.random.uniform(0,1.,(dim_action)).astype(np.float32)).reshape((1, -1))\n",
        "\n",
        "# v0.reshape((1, -1))\n",
        "linear_coff_s = np.ones((1,dim_action),dtype=np.float32)*(2)\n",
        "linear_coff_x = linear_coff= np.ones((1,dim_action),dtype=np.float32)*10\n",
        "\n",
        "x = init_state\n",
        "action_s = np.zeros((1,dim_action),dtype=np.float32)\n",
        "\n",
        "env.set_state(x)\n",
        "\n",
        "Trajectory_Linear.append(x)\n",
        "Trajectory_eta_Linear.append(env.state_eta)\n",
        "\n",
        "SimulationLength=760\n",
        "Record_u_Linear=[]\n",
        "Record_Loss_Linear=[]\n",
        "Record_action_network_Linear=[]\n",
        "Loss_Linear=0\n",
        "state_s = np.zeros((1,dim_action),dtype=np.float32)\n",
        "Record_up=[]\n",
        "Record_ui=[]\n",
        "record_grad_ui = []\n",
        "Trajectory_s_Linear.append(env.state_s)\n",
        "\n",
        "for i in range(SimulationLength):\n",
        "\n",
        "\n",
        "    id_model = 0\n",
        "    action_edgefeedback = Action_edge(env.state_eta, PI_NNMatrix_list[id_model], env)\n",
        "    u, up, ui = Action_NNMatrix(x, state_s,PI_NNMatrix_list[id_model], env)\n",
        "    next_state_s, next_state_v, next_state_eta, r, action_network= env.step_edge_WoCost(action_edgefeedback, up, ui)\n",
        "\n",
        "    Loss_Linear+=r\n",
        "    x = next_state_v\n",
        "    state_s = next_state_s\n",
        "    Trajectory_Linear.append(x)\n",
        "    Trajectory_eta_Linear.append(next_state_eta)\n",
        "    Trajectory_s_Linear.append(next_state_s)\n",
        "    Record_u_Linear.append(np.squeeze(u))\n",
        "    Record_up.append(np.squeeze(up))\n",
        "    Record_ui.append(np.squeeze(ui))\n",
        "    Record_action_network_Linear.append(np.squeeze(action_network))\n",
        "    Record_Loss_Linear.append(np.squeeze(r))\n",
        "    record_grad_ui.append(np.squeeze(env.calc_grad_action(ui)[1]))\n",
        "\n",
        "Trajectory_Linear = np.squeeze(np.asarray(Trajectory_Linear))\n",
        "Trajectory_s_Linear = np.squeeze(np.asarray(Trajectory_s_Linear))\n",
        "record_grad_ui = np.squeeze(np.asarray(record_grad_ui))\n",
        "Record_u_Linear = np.squeeze(np.asarray(Record_u_Linear))\n",
        "fig = plt.figure(figsize=(11,10), dpi=100)\n",
        "\n",
        "plt.subplot(4,2,1)\n",
        "\n",
        "\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_u_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action_total')\n",
        "\n",
        "plt.subplot(4,2,2)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_action_network_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action_network')\n",
        "\n",
        "plt.subplot(4,2,3)\n",
        "\n",
        "TimeRecord=np.arange(1,SimulationLength+2)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Trajectory_Linear)\n",
        "\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('speed')\n",
        "\n",
        "Trajectory_eta_Linear=np.squeeze(np.asarray(Trajectory_eta_Linear))\n",
        "\n",
        "plt.subplot(4,2,4)\n",
        "plt.plot(TimeRecord,Trajectory_eta_Linear)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('position')\n",
        "\n",
        "plt.subplot(4,2,5)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_up)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action u_p')\n",
        "\n",
        "\n",
        "plt.subplot(4,2,6)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Record_ui)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('Action u_i')\n",
        "fig.tight_layout()   \n",
        "\n",
        "plt.subplot(4,2,7)\n",
        "TimeRecord=np.arange(1,SimulationLength+2)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,Trajectory_s_Linear)\n",
        "\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('state_s')\n",
        "\n",
        "\n",
        "\n",
        "plt.subplot(4,2,8)\n",
        "TimeRecord=np.arange(1,SimulationLength+1)\n",
        "TimeRecord=env.delta_t*TimeRecord\n",
        "plt.plot(TimeRecord,record_grad_ui)\n",
        "plt.xlabel('time(s)')\n",
        "plt.ylabel('grad u_i')\n",
        "fig.tight_layout()   \n"
      ],
      "metadata": {
        "id": "Kv1hN9tVbaNH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}